{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIBLIOGRAFÍA:\n",
    "#[Kaggle dataset]: https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data\n",
    "#[Kaggle trabajado]: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/code\n",
    "#[Kaggle trabajado] : https://www.kaggle.com/code/devbilalkhan/advanced-predictive-analysis-heart-disease-uci/notebook#1.0.-Dataset-Inspection\n",
    "#[Revisar_Neuronal_Network]: https://www.kaggle.com/code/alexabelozerova/heart-disease-diagnostics-98-acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DESCRIPCIÓN DE VARIABLES: \n",
    "# [Enlace de algunas descripciones: https://www.kaggle.com/code/tumpanjawat/heart-attack-eda-cluster-8-ml-models/notebook#3-%7C-CORRELATION-]\n",
    "\n",
    "##### id: ID. Único para cada paciente. \n",
    "\n",
    "##### age: Edad. Este es un factor de riesgo clave para enfermedades del corazón. A medida que aumenta la edad, también lo hace el riesgo de arterias dañadas y estrechas, \n",
    "# debilitamiento o engrosamiento del músculo cardíaco y otros factores de riesgo de enfermedades cardíacas.\n",
    "\n",
    "##### dataset: n\n",
    "\n",
    "##### sex: Sexo (Male/Female). Los hombres generalmente tienen un mayor riesgo de enfermedades del corazón que las mujeres. \n",
    "# Sin embargo, después de la menopausia, el riesgo de una mujer aumenta hasta casi igualar al de un hombre.\n",
    "\n",
    "##### cp: Tipo de dolor en el pecho (['typical angina', 'atypical angina' , 'non-anginal', 'asymptomatic']). \n",
    "#El dolor en el pecho es un síntoma clave de enfermedad cardíaca.\n",
    "# Puede manifestarse de diferentes formas: angina típica, angina atípica, dolor no anginoso o incluso ser asintomático. \n",
    "# El dolor torácico asociado con enfermedad cardíaca suele describirse como molestia, pesadez, presión, dolor sordo, ardor, sensación de llenura, opresión o dolor intenso.\n",
    "\n",
    "##### trestbps: Presión arterial en reposo (en mm Hg al ingresar en hospital) . La presión arterial alta (hipertensión) puede endurecer y engrosar las arterias, lo que lleva \n",
    "# a una acumulación de placa (aterosclerosis)  que puede causar enfermedad de las arterias coronarias. La presión se mide en milímetros de mercurio (mm Hg) \n",
    "# y generalmente se registra como dos cifras. La presión arterial en reposo normal en un adulto es aproximadamente 120/80 mm Hg.\n",
    "\n",
    "##### chol: Colesterol en suero (en mg/dl)). El colesterol es un tipo de molécula lipídica. \n",
    "# Niveles altos de lipoproteína de baja densidad (LDL) o \"colesterol malo\" pueden aumentar el riesgo de enfermedades del corazón al formar placas y estrechar las arterias.\n",
    "\n",
    "##### fbs: Glucosa en ayunas (si se padece de diabetes > 120 mg/dl) ['True', 'False']. Los niveles altos de glucosa en sangre en ayunas (prediabetes o diabetes) \n",
    "# pueden contribuir al estrechamiento de las arterias  y aumentar el riesgo de enfermedad cardíaca. Un nivel de glucosa en sangre en ayunas menor de 100 mg/dL se considera normal. \n",
    "# Entre 100-125 mg/dL se considera prediabetes, y 126 mg/dL o más en dos pruebas separadas significa que tienes diabetes.\n",
    "\n",
    "##### restecg: Electrocardiograma en reposo. ['normal', 'stt abnormality', 'lv hypertrophy'] El ECG registra la actividad eléctrica del corazón y puede mostrar infartos \n",
    "# previos o problemas con el ritmo cardíaco. Resultados anormales pueden indicar condiciones cardíacas como hipertrofia ventricular izquierda o arritmias cardíacas.\n",
    "\n",
    "##### thalach: Frecuencia cardíaca máxima alcanzada. Durante pruebas de esfuerzo o ejercicio, la frecuencia cardíaca máxima puede indicar el estado físico cardiovascular \n",
    "# y la capacidad del corazón para manejar el esfuerzo.\n",
    "\n",
    "##### exang: Angina inducida por el ejercicio. ['True', 'False'] Esto ocurre cuando el músculo cardíaco no recibe tanta sangre (y por lo tanto oxígeno) como necesita \n",
    "# para el nivel de actividad física, causando dolor o molestia en el pecho.\n",
    "\n",
    "##### oldpeak: Depresión del Segmento ST Inducida por el Ejercicio en Relación al Reposo.(se mide en milivoltios) Los cambios en el segmento ST en un ECG pueden indicar enfermedad cardíaca. \n",
    "# La depresión del ST puede indicar isquemia, o falta de flujo sanguíneo suficiente al músculo cardíaco.\n",
    "#Valores normales: cerca de 0\n",
    "#Depresiones leves: 0.5–1.0 mV\n",
    "#Depresiones moderadas a severas: 2.0–4.0 mV\n",
    "#Casos graves (raros): puede llegar a >5.0 mV, pero más allá de 6 es extremadamente raro\n",
    "\n",
    "##### slope: Pendiente del Segmento ST en el Esfuerzo Máximo.['downsloping' 'flat' 'upsloping' nan]. La pendiente del segmento ST/frecuencia cardíaca (pendiente ST/FC), \n",
    "# se ha introducido como un índice de la demanda relativa de oxígeno del miocardio durante el ejercicio. La forma del segmento ST puede revelar mucho sobre el estado del corazón.\n",
    "\n",
    "##### ca: Número de Vasos Principales Visualizados por Fluoroscopía. [0-3] Esto mide la presencia de enfermedad en los vasos sanguíneos principales del corazón. Un número mayor \n",
    "# usualmente indica una enfermedad más severa.\n",
    "\n",
    "##### thal: Prueba de Estrés con Talio. [normal; fixed defect; reversible defect] Es un método de imagen nuclear que muestra qué tan bien fluye la \n",
    "# sangre al músculo cardíaco, tanto en reposo como durante la actividad. Puede revelar áreas del corazón que no están recibiendo suficiente sangre, \n",
    "# lo que indica enfermedad de las arterias coronarias.\n",
    "\n",
    "#### num: Resultado (Diagnóstico de Enfermedad Cardíaca). [0-4] Esta es la variable objetivo. Un valor de 0 indica un estrechamiento del diámetro menor al 50% \n",
    "# – no se considera una enfermedad cardíaca significativa, mientras que un valor de 1 indica un estrechamiento del diámetro mayor al 50% –\n",
    "# lo cual sí se considera una enfermedad cardíaca significativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La columna objetivo es num, que es el atributo a predecir. Usaremos esta columna para predecir la presencia de enfermedad cardíaca. \n",
    "# Los valores únicos en esta columna son: [0, 1, 2, 3, 4], lo que indica que hay 5 tipos de enfermedades cardíacas.\n",
    "#0 = Sin enfermedad cardíaca\n",
    "#1 = Enfermedad cardíaca leve\n",
    "#2 = Enfermedad cardíaca moderada\n",
    "#3 = Enfermedad cardíaca severa\n",
    "#4 = Enfermedad cardíaca crítica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El número de casos por origen es: Cleveland 303, Hungary 294, Switzerland 123 y Long Beach 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.IMPORTE LIBRERÍAS:\n",
    "\n",
    "#Manejo de datos\n",
    "import pandas as pd # data processing, CSV file\n",
    "import numpy as np\n",
    "\n",
    "#Visualización de datos\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns #statistical data visualization\n",
    "\n",
    "\n",
    "# Imputador -- rellenar valores nulos\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Preprocesamiento de los datos\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler #Categoric to Numeric\n",
    "\n",
    "#Fichero arff\n",
    "import arff\n",
    "\n",
    "#Selección de variables\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "#Machine Learning (Entrenamiento/Test)\n",
    "from sklearn.model_selection import train_test_split #Test/Train\n",
    "\n",
    "#Algoritmos clasificadores\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#Métricas\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.CARGA DEL DATASET Y EDA (ANÁLISIS EXPLORATORIO DE DATOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos el aspecto de las 5 primeras filas\n",
    "data_set = pd.read_csv(\"heart_disease_uci.csv\")\n",
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resumen de tipos de datos y valores nulos: número de columnas, número de filas, conteo de registros nulos y el tipo de la varibale. \n",
    "#Localizamos globalmente el tipo de variables que se toman. En esta base contamos con float, int y object.\n",
    "#Float=decimal; int=entero; object= categórico\n",
    "data_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones (filas, columnas)\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos estadísticas descriptivas para todas las columnas:\n",
    "#count: Número total de valores no nulos en la columna.\n",
    "#mean: Media aritmética de los valores.\n",
    "#std: Desviación estándar, que mide la dispersión de los datos respecto a la media.\n",
    "#min: Valor mínimo en la columna.\n",
    "#25% (Primer cuartil, Q1): El 25% de los datos es menor o igual a este valor.\n",
    "#50% (Mediana o Segundo cuartil, Q2): El 50% de los datos es menor o igual a este valor.\n",
    "#75% (Tercer cuartil, Q3): El 75% de los datos es menor o igual a este valor.\n",
    "#max: Valor máximo en la columna.\n",
    "\n",
    "#Las variables categóricas se añaden al incluir (include all). Estas son:\n",
    "#unique: Número de valores únicos en la columna.\n",
    "#top: El valor más frecuente en la columna.\n",
    "#freq: La frecuencia con la que aparece el valor más frecuente.\n",
    "\n",
    "\n",
    "data_set.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspeccionamos registros de las variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos las variables categóricas\n",
    "categorical_variables = data_set.select_dtypes(include=['object', 'category']).columns\n",
    "value_counts_dict = {}\n",
    "\n",
    "# Contador\n",
    "for variable in categorical_variables:\n",
    "    value_counts_dict[variable] = data_set[variable].value_counts()\n",
    "\n",
    "# Inicializamos una lista vacía\n",
    "data_list = []\n",
    "\n",
    "# Añadimos una tupla a la lista para cada categoría\n",
    "for variable, counts in value_counts_dict.items():\n",
    "    for category, count in counts.items():\n",
    "        data_list.append((variable, category, count))\n",
    "        \n",
    "# Añadimos una tupla vacía para crear un espacio entre variables\n",
    "    data_list.append((\"---\", \"---\", \"---\"))\n",
    "\n",
    "# Creación del data frame\n",
    "value_counts_df = pd.DataFrame(data_list, columns=['Variable', 'Category', 'Count'])\n",
    "\n",
    "# Creación del índice\n",
    "value_counts_df = value_counts_df.set_index('Variable')\n",
    "\n",
    "# Ejecutamos\n",
    "value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSERVACIONES:\n",
    "#En Machine Learning y Análisis de Datos se considera desbalance cuando una clase representa menos del 30% del total.\n",
    "\n",
    "#Desbalance en la distribución por sexo:\n",
    "#Existe un desbalance significativo en la variable 'sex', con 726 casos de 'Hombre' frente a 194 de 'Mujer'. \n",
    "#Esto sugiere que el conjunto de datos representa más a pacientes hombres, lo cual podría introducir un sesgo de género en cualquier análisis o modelo predictivo.\n",
    "\n",
    "#Variabilidad en el Tipo de Dolor en el Pecho (cp):\n",
    "#Dentro de la variable 'cp' (tipo de dolor en el pecho), hay un desbalance notable. El dolor asintomático es el más común con 496 apariciones, más del doble que el dolor no anginoso (204 apariciones) \n",
    "#y la angina atípica (174), y más de diez veces más común que la angina típica (46). Esta disparidad podría afectar la capacidad del modelo para predecir enfermedades cardíacas en los tipos de dolor \n",
    "#menos frecuentes debido a la menor cantidad de ejemplos para entrenar.\n",
    "\n",
    "#Glucosa en ayunas (fbs):\n",
    "#Hay desvalance notable entre las clases de tener diabétes o no. Esta disparidad podría afectar al estudio de predecir la enfermedad teniendo en cuenta esta variable.\n",
    "\n",
    "#Distribución de Resultados del Test de Talasemia (thal):\n",
    "#Al observar los resultados de la prueba de talasemia (thal), se ve una distribución relativamente equilibrada entre 'normal' (196) y 'defecto reversible' (192), pero el 'defecto fijo' \n",
    "#está subrepresentado con solo 46 casos. Este desbalance podría sesgar las predicciones del modelo hacia las categorías más representadas, afectando potencialmente la precisión diagnóstica \n",
    "#para pacientes con un 'defecto fijo'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspeccionamos los registros de la variable objetivo\n",
    "target = data_set['num'].value_counts()\n",
    "pd.DataFrame(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES:\n",
    "# La distribución de la variable 'num' en el conjunto de datos muestra un claro desbalance, con una mayoría de casos sin presencia de enfermedad cardíaca ('0': 411 casos) \n",
    "# y una cantidad progresivamente menor a medida que aumenta el nivel de severidad, siendo la severidad más alta ('4') la que tiene solo 28 casos.\n",
    "\n",
    "#Este desbalance puede sesgar al modelo hacia la predicción de ausencia de enfermedad, dificultando la detección de condiciones más severas debido a la escasez de ejemplos.\n",
    "\n",
    "#Por lo tanto, convertir este problema multiclase en una clasificación binaria (presencia vs. ausencia de enfermedad cardíaca) es una elección estratégica que simplifica la tarea, \n",
    "# potencialmente mejora el rendimiento del modelo y se enfoca en la distinción clínicamente importante de detectar enfermedad cardíaca.\n",
    "\n",
    "#Veremos esta transformación binaria más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar columnas irrelevantes\n",
    "df = data_set.drop(columns=['id', 'dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos una copia hasta este paso porque luego la necesitaremos para el gráfico de pastel. \n",
    "#Queremos que, antes de pasar categóricos a numéricos, se queden registrados los valores categóricos para que visualmente sea entendible en el gráfico de pastel\n",
    "df_cat=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos las columnas categóricas en numéricas\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "label_encoders = {}\n",
    "label_mappings = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Asegurarse de que la columna sea tipo objeto\n",
    "    df[col] = df[col].astype('object')\n",
    "    \n",
    "    # Identificar valores no nulos\n",
    "    mask = df[col].notnull()\n",
    "    \n",
    "    # Aplicar LabelEncoder solo a valores no nulos\n",
    "    df.loc[mask, col] = le.fit_transform(df.loc[mask, col])\n",
    "    \n",
    "    # Guardar encoder y mapping\n",
    "    label_encoders[col] = le\n",
    "    label_mappings[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "    # Convertir a int después de codificar\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "print(label_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Etiqueta binaria: 0 = sano, 1+ = enfermo\n",
    "#En este paso ya convertimos la columna objetivo en una binaria.\n",
    "df['num'] = (df['num'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspeccionamos los registros de la variable objetivo\n",
    "target = df['num'].value_counts()\n",
    "pd.DataFrame(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES\n",
    "#En este punto ya tendríamos todas las columnas convertidas en numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manejo de valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpieza correcta de valores nulos = Imputación de valores faltantes (Relleno)\n",
    "\n",
    "#Columnas Numéricas: Es un método de imputación que predice los valores faltantes de una columna basándose en otras columnas del DataFrame,\n",
    "# usando modelos estadísticos de forma iterativa. Es más preciso que simplemente usar la media o la mediana.\n",
    "#Columnas Categóricas: se utiliza la moda.\n",
    "#Columnas Binarios: se utiliza la moda.\n",
    "\n",
    "#De esta manera se mantiene estable el estudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES\n",
    "#Nos han salido valores nulos en las columnas ['trestbps','chol','fbs', 'restecg', 'thalch', 'exang', 'oldpeak', 'slope', 'ca', 'thal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos los valores nulos. Imputaciones.\n",
    "#Vamos a utilizar lo que hemos comentado anteriormente. Si la columna tiene menos de 4 valores únicos usamos moda, si tiene más utilizamos media.\n",
    "# Detectar columnas numéricas (previamente todas son ya numéricas)\n",
    "numeric_cols = df.select_dtypes(include=['int', 'float']).columns\n",
    "\n",
    "# Separar columnas dependiendo de la cardinalidad de clases:\n",
    "low_cardinality_cols = [col for col in numeric_cols if df[col].dropna().nunique() <= 4]\n",
    "high_cardinality_cols = [col for col in numeric_cols if col not in low_cardinality_cols]\n",
    "\n",
    "# Imputar columnas con pocos valores únicos usando la moda\n",
    "if low_cardinality_cols:\n",
    "    df[low_cardinality_cols] = SimpleImputer(strategy='most_frequent').fit_transform(df[low_cardinality_cols])\n",
    "\n",
    "# Imputar columnas continuas con la media\n",
    "if high_cardinality_cols:\n",
    "    df[high_cardinality_cols] = SimpleImputer(strategy='mean').fit_transform(df[high_cardinality_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobación de valores nulos\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valores duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos para la limpieza si hay filas completas duplicadas:\n",
    "df.loc[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como nos han salido valores duplicados, eliminamos las filas y dejamos una única.\n",
    "data = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobación de duplicados\n",
    "data.loc[data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.VISUALIZACIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de barras para cada variable\n",
    "data.hist(\n",
    "    figsize=(15, 10),\n",
    "    color='#E773B5',\n",
    "    alpha=0.4,\n",
    "    edgecolor='red'  # Borde para cada barra\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES:\n",
    "#Desde los histogramas hemos llegado a visualizar que para las variables binarias no podemos hacer una imputación con la media. Son valores booleanos. Lo correcto será aplicar la moda. \n",
    "#Esto implica que la precisión del modelo también se vea afectado y baje.\n",
    "\n",
    "#Hemos podido observar que para las columnas: 'restecg', 'slope', 'thal' se está tomando como valor numérico al valor NaN. Para poder ajustar esto correctamente, vamos a aplicar\n",
    "#en labelencoder que los valores NaN no sean transformados a numéricos.(aplicado en la celda de transformación de label encoder)\n",
    "\n",
    "#También observamos que para las columnas: restecg, slope, ca y thal, que tienen valores únicos, tras hacer el label encoder, observamos que hay un umbral de unicidad. No podemos hacer\n",
    "#la media para la imputacion en estos casos. Necesitamos que se ajuste bien utilizando la moda.\n",
    "\n",
    "#Los campos que son categóricos no aportan en gráfico de barras una comprensión completa. Las clases son representadas en datos numéricos.\n",
    "    #Para ver esto de forma adecuada vamos a utilizar gráficos de pastel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráfico de pastel para variables categóricas\n",
    "# Seleccionar columnas categóricas\n",
    "categorical_cols = df_cat.select_dtypes(include='object').columns\n",
    "\n",
    "# Determinar filas y columnas para subplots\n",
    "n = len(categorical_cols)\n",
    "cols = 3  # Número de columnas por fila\n",
    "rows = int(np.ceil(n / cols))\n",
    "\n",
    "# Crear figura y ejes\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols*5, rows*5))\n",
    "axes = axes.flatten()  # Convertir a lista para iterar fácilmente\n",
    "\n",
    "# Lista de colores en gama de rosa\n",
    "pink_palette = [\n",
    "    '#FDE2E4', '#F9C5D1', '#F7A1C4', '#F48FB1', '#F06292',\n",
    "    '#EC407A', '#E91E63', '#D81B60', '#C2185B', '#AD1457',\n",
    "    '#880E4F'\n",
    "]\n",
    "\n",
    "# Crear un gráfico de pastel por variable\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    \n",
    "    df_cat[col].value_counts().plot.pie(\n",
    "    ax=axes[i],\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    shadow=True,\n",
    "    explode=[0.05]*df_cat[col].nunique(),\n",
    "    colors=pink_palette[:df_cat[col].nunique()],  # Ajusta al número de categorías\n",
    "    textprops={'fontsize': 8}  # Tamaño de letra más chico\n",
    ")\n",
    "    axes[i].set_title(f'Distribución de {col}')\n",
    "    axes[i].set_ylabel('')\n",
    "\n",
    "# Ocultar los ejes extra si hay más subplots que variables\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estudio de valores atípicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Los outliers son datos que se encuentran muy alejados del resto de los valores en una distribución.\n",
    "#En estos casos tenemos que hacer limpieza de estos datos que generan ruido.\n",
    "#El estudio se suele contemplar para valores numéricos (aunque también se puede hacer para valores categóricos)\n",
    "#Por otro lado, no se pueden considerar outliers en variables binarias (0/1) o (True/False) porque estas variables solo tienen dos posibles valores \n",
    "#y no hay una noción de “valor extremo” como en variables continuas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solo vamos a tomar las columnas continuas (prescindiendo de las binarias y que tiene un umbral menor que 4, que no tiene sentido estudiarlas). La variable high_cardinality_cols está definida arriba\n",
    "# Crear un sub-dataframe solo con las columnas continuas\n",
    "df_continuous = data[high_cardinality_cols]\n",
    "\n",
    "#Detectar outliers usando IQR\n",
    "Q1 = df_continuous.quantile(0.25)\n",
    "Q3 = df_continuous.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "limite_inferior = Q1 - 1.5 * IQR\n",
    "limite_superior = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = (df_continuous < limite_inferior) | (df_continuous > limite_superior)\n",
    "\n",
    "# Mostrar cantidad de outliers por columna\n",
    "print(\"Número de outliers por columna:\")\n",
    "print(outliers.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualización valores atípicos. Diagrama de caja (Boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos el tamaño general del gráfico\n",
    "plt.figure(figsize=(18, 12))  # Más grande\n",
    "# Un boxplot por variable, distribuidos en 2 columnas por fila\n",
    "for i, col in enumerate(df_continuous.columns, 1):\n",
    "    plt.subplot((len(df_continuous.columns) + 1) // 2, 2, i)\n",
    "    plt.boxplot(df_continuous[col].dropna(), patch_artist=True,\n",
    "                boxprops=dict(facecolor='#E773B5', color='#E773B5', alpha=0.7),\n",
    "                medianprops=dict(color='white'),\n",
    "                whiskerprops=dict(color='#E773B5'),\n",
    "                capprops=dict(color='#E773B5'),\n",
    "                flierprops=dict(markerfacecolor='#E773B5', marker='o', alpha=0.5))\n",
    "    plt.title(col, fontsize=10)  \n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES\n",
    "\n",
    "#El ajuste de outliers depende de varios factores: si son valores posibles o reales dentro del contexto o si el modelo que vamos a estudiar no es sensible a outliers.\n",
    "#Casos donde conviene tratarlos: si son valores erroneos de entrada (no coincide con la realidad), si se usan modelos sensibles a outliers (como regresión lineal, KNN y SVM)\n",
    "\n",
    "#age: No tiene outliers. No tenemos que hacer ningún ajuste\n",
    "#trestbps (presión arterial en reposo): No tiene sentido que un valor para la presión arterial sea 0 mmHg, sin embargo para un intervalo 80-200 si.\n",
    "    # 80 mmHg puede representar la presión diastólica normal (cuando el corazón está en reposo).\n",
    "    # 200 mmHg puede representar una presión sistólica muy alta (como en casos de hipertensión severa).\n",
    "        #En este caso necesitamos hacer un ajuste para que el 0 no sea tomado como un valor real.\n",
    "#chol (colesterol): No tiene sentido que un valor para el colesterol sea 0 en la vida real, sin embargo sí puede tomar valor 600 mg/dL. Este caso sería extremadamente alto e indica condición\n",
    "#médica grave, pero no imposible\n",
    "        #En este caso necesitamos hacer un ajuste para que el 0 no sea tomado como valor real.\n",
    "#thalch: (frecuencia cardíaca máxima en una prueba de esfuerzo): Observamos que hay 2 valores atípicos que representan a registros cercanos a 60. Esto puede suceder si la persona no hizo\n",
    "#el esfuerzo suficiente por edad avanzada (efectivamente, tal y como hemos observado en la celda de abajo tras investigarlo). Por lo tanto, no tenemos que hacer ningún ajuste.\n",
    "#oldpeak (diferencia de nivel del segmento ST)\n",
    "        #En este caso necesitamos ajustar los valores negativos que no tienen sentido clínico.\n",
    "        #Revisamos además, los valores >6 que son valores muy altos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajuste valor atípico trestbps: \n",
    "#Vemos la cantidad de registros con valor 0 en el campo trestbps\n",
    "data[data['trestbps']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como solo tenemos un caso, podemos eliminarlo:\n",
    "data_drop = data[data['trestbps'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajuste valor atípico chol:\n",
    "#Vemos la cantidad de registros con valor 0 en el campo chol\n",
    "data_drop[data_drop['chol']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como son bastantes (171 registros) vamos a aplicar imputación tras convertir los datos de 0 a NaN:\n",
    "# Reemplazar ceros en 'chol' por NaN\n",
    "data_drop.loc[data_drop['chol'] == 0, 'chol'] = np.nan\n",
    "\n",
    "# Imputar con la media o mediana\n",
    "imputer = SimpleImputer(strategy='mean')  # o 'median'\n",
    "data_drop['chol'] = imputer.fit_transform(data_drop[['chol']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observación valor thalach:\n",
    "data_drop[data_drop['thalch'] <= 65]\n",
    "#Indicado en la observación de arriba: Es normal que los valores atípicos para la frecuencia cardíaca máxima estén por debajo de 65 bpm (pulsaciones) para estos casos donde las edades son avanzadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajuste valor oldpeak:\n",
    "#Vemos la cantidad de registros con valor negativo en el campo oldpeak\n",
    "data_drop[data_drop['oldpeak']<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tenemos 12 registros. Vamos a pasarlos a NaN e imputar\n",
    "data_drop.loc[data_drop['oldpeak'] < 0, 'oldpeak'] = np.nan\n",
    "# Imputar con la media o mediana\n",
    "imputer = SimpleImputer(strategy='mean')  # o 'median'\n",
    "data_drop['oldpeak'] = imputer.fit_transform(data_drop[['oldpeak']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos la cantidad de registros para valores mayores que 6\n",
    "data_drop[data_drop['oldpeak'] > 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como solo tenemos un caso, podemos eliminarlo:\n",
    "data_1 = data_drop[data_drop['oldpeak'] <= 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurate de usar el dataframe actualizado y solo las columnas numéricas continuas\n",
    "df_continuous = data_1[high_cardinality_cols]\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Un boxplot por variable\n",
    "for i, col in enumerate(df_continuous.columns, 1):\n",
    "    plt.subplot((len(df_continuous.columns) + 1) // 2, 2, i)\n",
    "    plt.boxplot(df_continuous[col].dropna(), patch_artist=True,\n",
    "                boxprops=dict(facecolor='#E773B5', color='#E773B5', alpha=0.7),\n",
    "                medianprops=dict(color='white'),\n",
    "                whiskerprops=dict(color='#E773B5'),\n",
    "                capprops=dict(color='#E773B5'),\n",
    "                flierprops=dict(markerfacecolor='#E773B5', marker='o', alpha=0.5))\n",
    "    plt.title(col, fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar DataFrame como ARFF manualmente\n",
    "def dataframe_to_arff(df, filename, relation_name=\"dataset\"):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f\"@RELATION {relation_name}\\n\\n\")\n",
    "        \n",
    "        for col in df.columns:\n",
    "            f.write(f\"@ATTRIBUTE {col} NUMERIC\\n\")\n",
    "        \n",
    "        f.write(\"\\n@DATA\\n\")\n",
    "        for _, row in df.iterrows():\n",
    "            f.write(\",\".join(map(str, row.tolist())) + \"\\n\")\n",
    "\n",
    "# Usar con tu DataFrame\n",
    "dataframe_to_arff(data_1, \"Heart_Attack_2.arff\")\n",
    "print(\"Archivo ARFF generado como 'Heart_Attack_2.arff'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Obtener la ruta a la carpeta Descargas\n",
    "descargas = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "\n",
    "# Guardar el CSV ahí\n",
    "df.to_csv(os.path.join(descargas, \"chatgpt2.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación\n",
    "correlation_matrix = data_1.corr(numeric_only=True)\n",
    "\n",
    "# Crear un colormap rosa personalizado\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "    \"rose_red_dark\",\n",
    "    ['#E0F8F1', '#ffffff', '#E773B5'],  # suave → medio → fuerte → oscuro\n",
    "    N=256\n",
    ")\n",
    "\n",
    "# Gráfico\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=cmap,\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.90}\n",
    ")\n",
    "plt.title('Matriz de Correlación', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES:\n",
    "#La matriz utilizada es la correlación de Pearson, que mide la realción lineal entre dos variables.\n",
    "#Los valores van:\n",
    "    #1.0 = correlación positiva perfecta (cuando una sube, la otra también lo hace en la misma proporción)\n",
    "    #0 = sin correlación lineal (no significa que no haya relación, solo que no es lineal)\n",
    "    #-1.0= corrlación negativa perfecta (cuando una sube, la otra baja en la misma proporción)\n",
    "#En nuestra matriz de correlación observamos que:\n",
    "#oldpeak y num con 0.4 tienen una correlación positiva moderada\n",
    "#oldpeak y exang con 0.4 tienen una correlación positiva moderada\n",
    "#exang y num con 0.43 tienen una correlación positiva moderada\n",
    "#cp y num con -0.39 tienen una correlación negatica moderada\n",
    "\n",
    "#Razones posibles que no inducen a correlaciones fuertes: (>0.7)\n",
    "#La relación no es lienal, pues la correlación de Pearson solo detecta correlaciones lineales. Si hay una relación curva, categórica o más compleja, no se verá reflejada\n",
    "#La variable num (variable objetivo) puede depender de múltiples factores combinados. Quizás ninguna variable por si sola tenga gran peso, pero en conjunto sí lo tienen(es típico en modelos de salud y medicina)\n",
    "#Si algunas variables son categóricas codificadas como números puede hacer que la correlación se vea afectada\n",
    "#El número de muestras es bajo\n",
    "#La limpieza no es completa\n",
    "\n",
    "#Pasos que podemos dar para comprobar la relación entre variables:\n",
    "#Visualizar relaciones no lineales (gráficos como pairplot,scatterplot o árboles de decisión)\n",
    "#Probar correlación de Spearman (detecta relaciones monótonas, no lineales)\n",
    "#Hacemos selección de variables (usando modelos como Randomforest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar características y etiquetas\n",
    "X = data_1.drop(columns='num')\n",
    "y = data_1['num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División en entrenamiento/prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES: Al subir el tamaño del test las precisiones son más altas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selector de variables\n",
    "#Tal y como hemos visto en las observaciones de la matriz de correlación, vamos a utilizar el método de selección de variables Randomforest para comprobar la relación entre variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar modelo\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "# Importancia de variables\n",
    "importancias = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "importancias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES: \n",
    "# Los números que han salido de cada variable representan el peso o contribución relativa de cada variable a la hora de hacer predicciones.\n",
    "#No están en escala absoluta (de 0 a 1). Entre todas ellas suman 1 y cuanto más alto, más útil es la variable para el modelo.\n",
    "# cp, thalch, chol, oldpeak son las más importantes según el modelo.\n",
    "# Podemos sacar como conclusión que la correlación no es identificador de la importancia predictiva en el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método \"Pairplot\" Modelo no lineal\n",
    "#Vamos a visualizar las variables más importante detectadas anteriormente con Randomforest a través de las gráficas pairplot (para detectar relaciones no lineales)\n",
    "# Seleccionamos las variables más importantes según Random Forest\n",
    "variables_interes = ['cp', 'thalch', 'chol', 'oldpeak', 'age', 'num']\n",
    "sns.pairplot(data_1[variables_interes], hue='num', palette='husl')\n",
    "plt.suptitle(\"Pairplot de variables importantes vs. 'num'\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES:\n",
    "#Efectivamente podemos observar que entre variables no se dan relaciones lineales, se dan curvas y agrupamientos.\n",
    "#num = 0 → Cian y num = 1 → Rosa\n",
    "#Cada fila y columna es una variable (las seleccionadas por importancia del Random Forest)\n",
    "#Las gráficas de la diagonal son distribuciones (KDEs)\n",
    "#Las otras son scatterplots (gráfico de puntos)\n",
    "\n",
    "#Desglose visualizaciones importantes por por variable:\n",
    "#cp (tipo de dolor de pecho)\n",
    "#Distribución muy diferente entre clases → esto no lo vería la correlación lineal, pero sí Random Forest.\n",
    "\n",
    "#oldpeak (descenso de ST)\n",
    "#num = 1 se agrupa más en valores altos de oldpeak.\n",
    "#Es decir, cuanto mayor es el descenso de ST (oldpeak), más probable es que tenga enfermedad.\n",
    "#Esto también es no lineal, ya que no hay una relación directa, pero sí una zona crítica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de modelos\n",
    "models = {\n",
    "    'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "    'SVM': make_pipeline(StandardScaler(), SVC()),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'C4.5': DecisionTreeClassifier(criterion='entropy'),\n",
    "    'Neural Network': make_pipeline(\n",
    "        StandardScaler(),\n",
    "        MLPClassifier(hidden_layer_sizes=(30,), max_iter=500, random_state=42)\n",
    "    ),\n",
    "    'Bagging': BaggingClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'Stacking': StackingClassifier(\n",
    "        estimators=[\n",
    "            ('knn', make_pipeline(StandardScaler(), KNeighborsClassifier())),\n",
    "            ('svm', make_pipeline(StandardScaler(), SVC(probability=True))),\n",
    "            ('nb', GaussianNB())\n",
    "        ],\n",
    "        final_estimator=make_pipeline(StandardScaler(), LogisticRegression())\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamiento y evaluación\n",
    "accuracy_scores = {}\n",
    "conf_matrices = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy_scores[name] = accuracy_score(y_test, y_pred)\n",
    "    conf_matrices[name] = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"{name} Accuracy: {accuracy_scores[name]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a tomar entonces para el estudio de la precisión de los modelos solo las variables que hemos seleccionado por el método Randomforest\n",
    "#Comprobemos el comportamiento de la precisiones de los algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_interes = ['cp', 'thalch', 'chol', 'oldpeak', 'age', 'num']\n",
    "data_2 = data_1[variables_interes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar características y etiquetas\n",
    "A = data_2.drop(columns='num')\n",
    "b = data_2['num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División en entrenamiento/prueba\n",
    "A_train, A_test, b_train, b_test = train_test_split(A, b, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamiento y evaluación\n",
    "accuracy_scores_red = {}\n",
    "conf_matrices = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(A_train, b_train)\n",
    "    b_pred = model.predict(A_test)\n",
    "    accuracy_scores_red[name] = accuracy_score(b_test, b_pred)\n",
    "    conf_matrices[name] = confusion_matrix(b_test, b_pred)\n",
    "\n",
    "    print(f\"{name} Accuracy: {accuracy_scores_red[name]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES:\n",
    "#El modelo KNN subió bastante. Esto es esperable, porque KNN se ve afectado por ruido y variables irrelevantes\n",
    "#C4.5 y AdaBoost también mejoraron. Mismos motivos que el caso anterior.\n",
    "#NaiveBayes y Stacking tuvieron pequeña pérdida de precisión y esto se debe, no necesariamente a algo malo, ya que al reducir variables el modelo es más simple.\n",
    "#Estos modelos necesitan gran cantidad de datos para su estudio. \n",
    "#SVM se mantiene igual, lo que indica que es bastante bueno pues los atributos no aportaban al margen de decisión de SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "modelos = list(accuracy_scores.keys())\n",
    "todas_vals = [accuracy_scores[m] for m in modelos]\n",
    "reducidas_vals = [accuracy_scores_red[m] for m in modelos]\n",
    "\n",
    "# Crear gráfico\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.35\n",
    "index = range(len(modelos))\n",
    "\n",
    "plt.bar(index, todas_vals, bar_width, label='Todas las variables', color='#F08AC0')\n",
    "plt.bar([i + bar_width for i in index], reducidas_vals, bar_width, label='Variables seleccionadas', color='#D63384')\n",
    "\n",
    "plt.xlabel('Modelo')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparación de precisión por modelo')\n",
    "plt.xticks([i + bar_width / 2 for i in index], modelos, rotation=45)\n",
    "plt.ylim(0.6, 0.85)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método de validación cruzada\n",
    "#Vamos a aplicar este método para comparar con lo anterior.\n",
    "#El método de validación cruzada es un método para medir la precisión de forma más robusta. Es método que sustituye al paso clásico de (train/test).\n",
    "#El método train/test da una única precisión (en test) es decir, tiene una única división y puede ser engañosa si el test es difícil o fácil\n",
    "#El método de validación cruzada entrena y valida K-veces y devuelve el promedio en la precisión. Esto hace que el método sea más robusto y fiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Método de validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular validación cruzada\n",
    "accuracy_todas = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    accuracy_todas[name] = scores.mean()\n",
    "\n",
    "# Mostrar resultados ordenados\n",
    "print(\"📊 Accuracy promedio con TODAS las variables:\")\n",
    "for modelo, acc in sorted(accuracy_todas.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{modelo:15s}: {acc * 100:.2f}%\")\n",
    "\n",
    "# (Opcional) Graficar resultados\n",
    "modelos = list(accuracy_todas.keys())\n",
    "valores = [accuracy_todas[m] for m in modelos]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(modelos, valores, color='#E773B5')\n",
    "plt.ylabel(\"Accuracy promedio\")\n",
    "plt.title(\"Validación cruzada (5-Fold) con todas las variables\")\n",
    "plt.ylim(0.6, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular validación cruzada\n",
    "accuracy_todas_red = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, A, b, cv=5)\n",
    "    accuracy_todas_red[name] = scores.mean()\n",
    "\n",
    "# Mostrar resultados ordenados\n",
    "print(\"📊 Accuracy promedio con las variables seleccionadas:\")\n",
    "for modelo, acc in sorted(accuracy_todas_red.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{modelo:15s}: {acc * 100:.2f}%\")\n",
    "\n",
    "# (Opcional) Graficar resultados\n",
    "modelos = list(accuracy_todas_red.keys())\n",
    "valores = [accuracy_todas_red[m] for m in modelos]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(modelos, valores, color='#E773B5')\n",
    "plt.ylabel(\"Accuracy promedio\")\n",
    "plt.title(\"Validación cruzada (5-Fold) con las variables seleccionadas\")\n",
    "plt.ylim(0.6, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "modelos = list(accuracy_todas.keys())\n",
    "todas_vals = [accuracy_todas[m] for m in modelos]\n",
    "reducidas_vals = [accuracy_todas_red[m] for m in modelos]\n",
    "\n",
    "# Crear gráfico\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.35\n",
    "index = range(len(modelos))\n",
    "\n",
    "plt.bar(index, todas_vals, bar_width, label='Todas las variables', color='#F08AC0')\n",
    "plt.bar([i + bar_width for i in index], reducidas_vals, bar_width, label='Variables seleccionadas', color='#D63384')\n",
    "\n",
    "plt.xlabel('Modelo')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparación de precisión por modelo. Validación cruzada')\n",
    "plt.xticks([i + bar_width / 2 for i in index], modelos, rotation=45)\n",
    "plt.ylim(0.6, 0.85)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES:\n",
    "#Las observaciones para el método de validación cruzada son más realistas y menos optimistas del rendimiento del modelo. Representan mejor el comportamiento en general.\n",
    "#Representa el rendimiento real del modelo\n",
    "#Que las precisiones hayan bajado es porque ha evitado el sobreajuste al modelo de train/test. \n",
    "#Con validación cruzada el modelo se evalua varias veces sobre diferenes particiones, eliminando la suerte ocasional.\n",
    "#Reduce el riesgo de overfiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráfica de comparación de precisión por modelo:Train/Test vs Validación Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = list(accuracy_scores.keys())\n",
    "x = range(len(modelos))\n",
    "bar_width = 0.2\n",
    "\n",
    "# Crear gráfico combinado\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Train/test\n",
    "plt.bar([i - 1.5 * bar_width for i in x], [accuracy_scores[m] for m in modelos],\n",
    "        width=bar_width, label='Train/Test - Todas', color='#A4C8E1')\n",
    "\n",
    "plt.bar([i - 0.5 * bar_width for i in x], [accuracy_scores_red[m] for m in modelos],\n",
    "        width=bar_width, label='Train/Test - Seleccionadas', color='#6497B1')\n",
    "\n",
    "# Validación cruzada\n",
    "plt.bar([i + 0.5 * bar_width for i in x], [accuracy_todas[m] for m in modelos],\n",
    "        width=bar_width, label='CV - Todas', color='#F08AC0')\n",
    "\n",
    "plt.bar([i + 1.5 * bar_width for i in x], [accuracy_todas_red[m] for m in modelos],\n",
    "        width=bar_width, label='CV - Seleccionadas', color='#D63384')\n",
    "\n",
    "# Ajustes\n",
    "plt.xlabel('Modelo')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparación de precisión por modelo: Train/Test vs Validación Cruzada')\n",
    "plt.xticks(x, modelos, rotation=45)\n",
    "plt.ylim(0.6, 0.85)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES:\n",
    "#Modelos árbol como C4.5 y Bagging tienen a ajustarse al dataset de entrenamiento si no se podan o se regularizan bien. Por eso, cuando se prueban en validación cruzada, con varias particiones,\n",
    "#su rendimiento cae más.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consideraciones generales (Modelos Algoritmos)\n",
    "# Porcentaje de test/train:\n",
    "    # Lo estándar es 80% train / 20% test o usar validación cruzada (cv=5 o cv=10).\n",
    "    # Modelos como Neural Network y SVM se benefician de más datos para entrenar.\n",
    "\n",
    "# Tratamiento de outliers:\n",
    "    # Muy importante para KNN y redes neuronales.\n",
    "    # Árboles y bagging los manejan mejor por diseño.\n",
    "\n",
    "# Ruido en los datos:\n",
    "    # KNN, redes y árboles lo sufren más.\n",
    "    # Bagging y Naive Bayes lo toleran mejor.\n",
    "\n",
    "# Normalización:\n",
    "    # Crucial para todos los algoritmos basados en distancia o gradientes: KNN, SVM, Redes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

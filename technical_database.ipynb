{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIBLIOGRAF√çA:\n",
    "#[Kaggle dataset]: https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data\n",
    "#[Kaggle trabajado]: https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/code\n",
    "#[Kaggle trabajado] : https://www.kaggle.com/code/devbilalkhan/advanced-predictive-analysis-heart-disease-uci/notebook#1.0.-Dataset-Inspection\n",
    "#[Revisar_Neuronal_Network]: https://www.kaggle.com/code/alexabelozerova/heart-disease-diagnostics-98-acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DESCRIPCI√ìN DE VARIABLES: \n",
    "# [Enlace de algunas descripciones: https://www.kaggle.com/code/tumpanjawat/heart-attack-eda-cluster-8-ml-models/notebook#3-%7C-CORRELATION-]\n",
    "\n",
    "##### id: ID. √önico para cada paciente. \n",
    "\n",
    "##### age: Edad. Este es un factor de riesgo clave para enfermedades del coraz√≥n. A medida que aumenta la edad, tambi√©n lo hace el riesgo de arterias da√±adas y estrechas, \n",
    "# debilitamiento o engrosamiento del m√∫sculo card√≠aco y otros factores de riesgo de enfermedades card√≠acas.\n",
    "\n",
    "##### dataset: n\n",
    "\n",
    "##### sex: Sexo (Male/Female). Los hombres generalmente tienen un mayor riesgo de enfermedades del coraz√≥n que las mujeres. \n",
    "# Sin embargo, despu√©s de la menopausia, el riesgo de una mujer aumenta hasta casi igualar al de un hombre.\n",
    "\n",
    "##### cp: Tipo de dolor en el pecho (['typical angina', 'atypical angina' , 'non-anginal', 'asymptomatic']). \n",
    "#El dolor en el pecho es un s√≠ntoma clave de enfermedad card√≠aca.\n",
    "# Puede manifestarse de diferentes formas: angina t√≠pica, angina at√≠pica, dolor no anginoso o incluso ser asintom√°tico. \n",
    "# El dolor tor√°cico asociado con enfermedad card√≠aca suele describirse como molestia, pesadez, presi√≥n, dolor sordo, ardor, sensaci√≥n de llenura, opresi√≥n o dolor intenso.\n",
    "\n",
    "##### trestbps: Presi√≥n arterial en reposo (en mm Hg al ingresar en hospital) . La presi√≥n arterial alta (hipertensi√≥n) puede endurecer y engrosar las arterias, lo que lleva \n",
    "# a una acumulaci√≥n de placa (aterosclerosis)  que puede causar enfermedad de las arterias coronarias. La presi√≥n se mide en mil√≠metros de mercurio (mm Hg) \n",
    "# y generalmente se registra como dos cifras. La presi√≥n arterial en reposo normal en un adulto es aproximadamente 120/80 mm Hg.\n",
    "\n",
    "##### chol: Colesterol en suero (en mg/dl)). El colesterol es un tipo de mol√©cula lip√≠dica. \n",
    "# Niveles altos de lipoprote√≠na de baja densidad (LDL) o \"colesterol malo\" pueden aumentar el riesgo de enfermedades del coraz√≥n al formar placas y estrechar las arterias.\n",
    "\n",
    "##### fbs: Glucosa en ayunas (si se padece de diabetes > 120 mg/dl) ['True', 'False']. Los niveles altos de glucosa en sangre en ayunas (prediabetes o diabetes) \n",
    "# pueden contribuir al estrechamiento de las arterias  y aumentar el riesgo de enfermedad card√≠aca. Un nivel de glucosa en sangre en ayunas menor de 100 mg/dL se considera normal. \n",
    "# Entre 100-125 mg/dL se considera prediabetes, y 126 mg/dL o m√°s en dos pruebas separadas significa que tienes diabetes.\n",
    "\n",
    "##### restecg: Electrocardiograma en reposo. ['normal', 'stt abnormality', 'lv hypertrophy'] El ECG registra la actividad el√©ctrica del coraz√≥n y puede mostrar infartos \n",
    "# previos o problemas con el ritmo card√≠aco. Resultados anormales pueden indicar condiciones card√≠acas como hipertrofia ventricular izquierda o arritmias card√≠acas.\n",
    "\n",
    "##### thalach: Frecuencia card√≠aca m√°xima alcanzada. Durante pruebas de esfuerzo o ejercicio, la frecuencia card√≠aca m√°xima puede indicar el estado f√≠sico cardiovascular \n",
    "# y la capacidad del coraz√≥n para manejar el esfuerzo.\n",
    "\n",
    "##### exang: Angina inducida por el ejercicio. ['True', 'False'] Esto ocurre cuando el m√∫sculo card√≠aco no recibe tanta sangre (y por lo tanto ox√≠geno) como necesita \n",
    "# para el nivel de actividad f√≠sica, causando dolor o molestia en el pecho.\n",
    "\n",
    "##### oldpeak: Depresi√≥n del Segmento ST Inducida por el Ejercicio en Relaci√≥n al Reposo.(se mide en milivoltios) Los cambios en el segmento ST en un ECG pueden indicar enfermedad card√≠aca. \n",
    "# La depresi√≥n del ST puede indicar isquemia, o falta de flujo sangu√≠neo suficiente al m√∫sculo card√≠aco.\n",
    "#Valores normales: cerca de 0\n",
    "#Depresiones leves: 0.5‚Äì1.0 mV\n",
    "#Depresiones moderadas a severas: 2.0‚Äì4.0 mV\n",
    "#Casos graves (raros): puede llegar a >5.0 mV, pero m√°s all√° de 6 es extremadamente raro\n",
    "\n",
    "##### slope: Pendiente del Segmento ST en el Esfuerzo M√°ximo.['downsloping' 'flat' 'upsloping' nan]. La pendiente del segmento ST/frecuencia card√≠aca (pendiente ST/FC), \n",
    "# se ha introducido como un √≠ndice de la demanda relativa de ox√≠geno del miocardio durante el ejercicio. La forma del segmento ST puede revelar mucho sobre el estado del coraz√≥n.\n",
    "\n",
    "##### ca: N√∫mero de Vasos Principales Visualizados por Fluoroscop√≠a. [0-3] Esto mide la presencia de enfermedad en los vasos sangu√≠neos principales del coraz√≥n. Un n√∫mero mayor \n",
    "# usualmente indica una enfermedad m√°s severa.\n",
    "\n",
    "##### thal: Prueba de Estr√©s con Talio. [normal; fixed defect; reversible defect] Es un m√©todo de imagen nuclear que muestra qu√© tan bien fluye la \n",
    "# sangre al m√∫sculo card√≠aco, tanto en reposo como durante la actividad. Puede revelar √°reas del coraz√≥n que no est√°n recibiendo suficiente sangre, \n",
    "# lo que indica enfermedad de las arterias coronarias.\n",
    "\n",
    "#### num: Resultado (Diagn√≥stico de Enfermedad Card√≠aca). [0-4] Esta es la variable objetivo. Un valor de 0 indica un estrechamiento del di√°metro menor al 50% \n",
    "# ‚Äì no se considera una enfermedad card√≠aca significativa, mientras que un valor de 1 indica un estrechamiento del di√°metro mayor al 50% ‚Äì\n",
    "# lo cual s√≠ se considera una enfermedad card√≠aca significativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La columna objetivo es num, que es el atributo a predecir. Usaremos esta columna para predecir la presencia de enfermedad card√≠aca. \n",
    "# Los valores √∫nicos en esta columna son: [0, 1, 2, 3, 4], lo que indica que hay 5 tipos de enfermedades card√≠acas.\n",
    "#0 = Sin enfermedad card√≠aca\n",
    "#1 = Enfermedad card√≠aca leve\n",
    "#2 = Enfermedad card√≠aca moderada\n",
    "#3 = Enfermedad card√≠aca severa\n",
    "#4 = Enfermedad card√≠aca cr√≠tica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El n√∫mero de casos por origen es: Cleveland 303, Hungary 294, Switzerland 123 y Long Beach 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.IMPORTE LIBRER√çAS:\n",
    "\n",
    "#Manejo de datos\n",
    "import pandas as pd # data processing, CSV file\n",
    "import numpy as np\n",
    "\n",
    "#Visualizaci√≥n de datos\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns #statistical data visualization\n",
    "\n",
    "\n",
    "# Imputador -- rellenar valores nulos\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Preprocesamiento de los datos\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler #Categoric to Numeric\n",
    "\n",
    "#Fichero arff\n",
    "import arff\n",
    "\n",
    "#Selecci√≥n de variables\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "#Machine Learning (Entrenamiento/Test)\n",
    "from sklearn.model_selection import train_test_split #Test/Train\n",
    "\n",
    "#Algoritmos clasificadores\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#M√©tricas\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.CARGA DEL DATASET Y EDA (AN√ÅLISIS EXPLORATORIO DE DATOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos el aspecto de las 5 primeras filas\n",
    "data_set = pd.read_csv(\"heart_disease_uci.csv\")\n",
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resumen de tipos de datos y valores nulos: n√∫mero de columnas, n√∫mero de filas, conteo de registros nulos y el tipo de la varibale. \n",
    "#Localizamos globalmente el tipo de variables que se toman. En esta base contamos con float, int y object.\n",
    "#Float=decimal; int=entero; object= categ√≥rico\n",
    "data_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones (filas, columnas)\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos estad√≠sticas descriptivas para todas las columnas:\n",
    "#count: N√∫mero total de valores no nulos en la columna.\n",
    "#mean: Media aritm√©tica de los valores.\n",
    "#std: Desviaci√≥n est√°ndar, que mide la dispersi√≥n de los datos respecto a la media.\n",
    "#min: Valor m√≠nimo en la columna.\n",
    "#25% (Primer cuartil, Q1): El 25% de los datos es menor o igual a este valor.\n",
    "#50% (Mediana o Segundo cuartil, Q2): El 50% de los datos es menor o igual a este valor.\n",
    "#75% (Tercer cuartil, Q3): El 75% de los datos es menor o igual a este valor.\n",
    "#max: Valor m√°ximo en la columna.\n",
    "\n",
    "#Las variables categ√≥ricas se a√±aden al incluir (include all). Estas son:\n",
    "#unique: N√∫mero de valores √∫nicos en la columna.\n",
    "#top: El valor m√°s frecuente en la columna.\n",
    "#freq: La frecuencia con la que aparece el valor m√°s frecuente.\n",
    "\n",
    "\n",
    "data_set.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspeccionamos registros de las variables categ√≥ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos las variables categ√≥ricas\n",
    "categorical_variables = data_set.select_dtypes(include=['object', 'category']).columns\n",
    "value_counts_dict = {}\n",
    "\n",
    "# Contador\n",
    "for variable in categorical_variables:\n",
    "    value_counts_dict[variable] = data_set[variable].value_counts()\n",
    "\n",
    "# Inicializamos una lista vac√≠a\n",
    "data_list = []\n",
    "\n",
    "# A√±adimos una tupla a la lista para cada categor√≠a\n",
    "for variable, counts in value_counts_dict.items():\n",
    "    for category, count in counts.items():\n",
    "        data_list.append((variable, category, count))\n",
    "        \n",
    "# A√±adimos una tupla vac√≠a para crear un espacio entre variables\n",
    "    data_list.append((\"---\", \"---\", \"---\"))\n",
    "\n",
    "# Creaci√≥n del data frame\n",
    "value_counts_df = pd.DataFrame(data_list, columns=['Variable', 'Category', 'Count'])\n",
    "\n",
    "# Creaci√≥n del √≠ndice\n",
    "value_counts_df = value_counts_df.set_index('Variable')\n",
    "\n",
    "# Ejecutamos\n",
    "value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSERVACIONES:\n",
    "#En Machine Learning y An√°lisis de Datos se considera desbalance cuando una clase representa menos del 30% del total.\n",
    "\n",
    "#Desbalance en la distribuci√≥n por sexo:\n",
    "#Existe un desbalance significativo en la variable 'sex', con 726 casos de 'Hombre' frente a 194 de 'Mujer'. \n",
    "#Esto sugiere que el conjunto de datos representa m√°s a pacientes hombres, lo cual podr√≠a introducir un sesgo de g√©nero en cualquier an√°lisis o modelo predictivo.\n",
    "\n",
    "#Variabilidad en el Tipo de Dolor en el Pecho (cp):\n",
    "#Dentro de la variable 'cp' (tipo de dolor en el pecho), hay un desbalance notable. El dolor asintom√°tico es el m√°s com√∫n con 496 apariciones, m√°s del doble que el dolor no anginoso (204 apariciones) \n",
    "#y la angina at√≠pica (174), y m√°s de diez veces m√°s com√∫n que la angina t√≠pica (46). Esta disparidad podr√≠a afectar la capacidad del modelo para predecir enfermedades card√≠acas en los tipos de dolor \n",
    "#menos frecuentes debido a la menor cantidad de ejemplos para entrenar.\n",
    "\n",
    "#Glucosa en ayunas (fbs):\n",
    "#Hay desvalance notable entre las clases de tener diab√©tes o no. Esta disparidad podr√≠a afectar al estudio de predecir la enfermedad teniendo en cuenta esta variable.\n",
    "\n",
    "#Distribuci√≥n de Resultados del Test de Talasemia (thal):\n",
    "#Al observar los resultados de la prueba de talasemia (thal), se ve una distribuci√≥n relativamente equilibrada entre 'normal' (196) y 'defecto reversible' (192), pero el 'defecto fijo' \n",
    "#est√° subrepresentado con solo 46 casos. Este desbalance podr√≠a sesgar las predicciones del modelo hacia las categor√≠as m√°s representadas, afectando potencialmente la precisi√≥n diagn√≥stica \n",
    "#para pacientes con un 'defecto fijo'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspeccionamos los registros de la variable objetivo\n",
    "target = data_set['num'].value_counts()\n",
    "pd.DataFrame(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES:\n",
    "# La distribuci√≥n de la variable 'num' en el conjunto de datos muestra un claro desbalance, con una mayor√≠a de casos sin presencia de enfermedad card√≠aca ('0': 411 casos) \n",
    "# y una cantidad progresivamente menor a medida que aumenta el nivel de severidad, siendo la severidad m√°s alta ('4') la que tiene solo 28 casos.\n",
    "\n",
    "#Este desbalance puede sesgar al modelo hacia la predicci√≥n de ausencia de enfermedad, dificultando la detecci√≥n de condiciones m√°s severas debido a la escasez de ejemplos.\n",
    "\n",
    "#Por lo tanto, convertir este problema multiclase en una clasificaci√≥n binaria (presencia vs. ausencia de enfermedad card√≠aca) es una elecci√≥n estrat√©gica que simplifica la tarea, \n",
    "# potencialmente mejora el rendimiento del modelo y se enfoca en la distinci√≥n cl√≠nicamente importante de detectar enfermedad card√≠aca.\n",
    "\n",
    "#Veremos esta transformaci√≥n binaria m√°s adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar columnas irrelevantes\n",
    "df = data_set.drop(columns=['id', 'dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos una copia hasta este paso porque luego la necesitaremos para el gr√°fico de pastel. \n",
    "#Queremos que, antes de pasar categ√≥ricos a num√©ricos, se queden registrados los valores categ√≥ricos para que visualmente sea entendible en el gr√°fico de pastel\n",
    "df_cat=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos las columnas categ√≥ricas en num√©ricas\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "\n",
    "label_encoders = {}\n",
    "label_mappings = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Asegurarse de que la columna sea tipo objeto\n",
    "    df[col] = df[col].astype('object')\n",
    "    \n",
    "    # Identificar valores no nulos\n",
    "    mask = df[col].notnull()\n",
    "    \n",
    "    # Aplicar LabelEncoder solo a valores no nulos\n",
    "    df.loc[mask, col] = le.fit_transform(df.loc[mask, col])\n",
    "    \n",
    "    # Guardar encoder y mapping\n",
    "    label_encoders[col] = le\n",
    "    label_mappings[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "    # Convertir a int despu√©s de codificar\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "print(label_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Etiqueta binaria: 0 = sano, 1+ = enfermo\n",
    "#En este paso ya convertimos la columna objetivo en una binaria.\n",
    "df['num'] = (df['num'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspeccionamos los registros de la variable objetivo\n",
    "target = df['num'].value_counts()\n",
    "pd.DataFrame(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES\n",
    "#En este punto ya tendr√≠amos todas las columnas convertidas en num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manejo de valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpieza correcta de valores nulos = Imputaci√≥n de valores faltantes (Relleno)\n",
    "\n",
    "#Columnas Num√©ricas: Es un m√©todo de imputaci√≥n que predice los valores faltantes de una columna bas√°ndose en otras columnas del DataFrame,\n",
    "# usando modelos estad√≠sticos de forma iterativa. Es m√°s preciso que simplemente usar la media o la mediana.\n",
    "#Columnas Categ√≥ricas: se utiliza la moda.\n",
    "#Columnas Binarios: se utiliza la moda.\n",
    "\n",
    "#De esta manera se mantiene estable el estudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES\n",
    "#Nos han salido valores nulos en las columnas ['trestbps','chol','fbs', 'restecg', 'thalch', 'exang', 'oldpeak', 'slope', 'ca', 'thal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos los valores nulos. Imputaciones.\n",
    "#Vamos a utilizar lo que hemos comentado anteriormente. Si la columna tiene menos de 4 valores √∫nicos usamos moda, si tiene m√°s utilizamos media.\n",
    "# Detectar columnas num√©ricas (previamente todas son ya num√©ricas)\n",
    "numeric_cols = df.select_dtypes(include=['int', 'float']).columns\n",
    "\n",
    "# Separar columnas dependiendo de la cardinalidad de clases:\n",
    "low_cardinality_cols = [col for col in numeric_cols if df[col].dropna().nunique() <= 4]\n",
    "high_cardinality_cols = [col for col in numeric_cols if col not in low_cardinality_cols]\n",
    "\n",
    "# Imputar columnas con pocos valores √∫nicos usando la moda\n",
    "if low_cardinality_cols:\n",
    "    df[low_cardinality_cols] = SimpleImputer(strategy='most_frequent').fit_transform(df[low_cardinality_cols])\n",
    "\n",
    "# Imputar columnas continuas con la media\n",
    "if high_cardinality_cols:\n",
    "    df[high_cardinality_cols] = SimpleImputer(strategy='mean').fit_transform(df[high_cardinality_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobaci√≥n de valores nulos\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valores duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos para la limpieza si hay filas completas duplicadas:\n",
    "df.loc[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como nos han salido valores duplicados, eliminamos las filas y dejamos una √∫nica.\n",
    "data = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobaci√≥n de duplicados\n",
    "data.loc[data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.VISUALIZACIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°fico de barras para cada variable\n",
    "data.hist(\n",
    "    figsize=(15, 10),\n",
    "    color='#E773B5',\n",
    "    alpha=0.4,\n",
    "    edgecolor='red'  # Borde para cada barra\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES:\n",
    "#Desde los histogramas hemos llegado a visualizar que para las variables binarias no podemos hacer una imputaci√≥n con la media. Son valores booleanos. Lo correcto ser√° aplicar la moda. \n",
    "#Esto implica que la precisi√≥n del modelo tambi√©n se vea afectado y baje.\n",
    "\n",
    "#Hemos podido observar que para las columnas: 'restecg', 'slope', 'thal' se est√° tomando como valor num√©rico al valor NaN. Para poder ajustar esto correctamente, vamos a aplicar\n",
    "#en labelencoder que los valores NaN no sean transformados a num√©ricos.(aplicado en la celda de transformaci√≥n de label encoder)\n",
    "\n",
    "#Tambi√©n observamos que para las columnas: restecg, slope, ca y thal, que tienen valores √∫nicos, tras hacer el label encoder, observamos que hay un umbral de unicidad. No podemos hacer\n",
    "#la media para la imputacion en estos casos. Necesitamos que se ajuste bien utilizando la moda.\n",
    "\n",
    "#Los campos que son categ√≥ricos no aportan en gr√°fico de barras una comprensi√≥n completa. Las clases son representadas en datos num√©ricos.\n",
    "    #Para ver esto de forma adecuada vamos a utilizar gr√°ficos de pastel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gr√°fico de pastel para variables categ√≥ricas\n",
    "# Seleccionar columnas categ√≥ricas\n",
    "categorical_cols = df_cat.select_dtypes(include='object').columns\n",
    "\n",
    "# Determinar filas y columnas para subplots\n",
    "n = len(categorical_cols)\n",
    "cols = 3  # N√∫mero de columnas por fila\n",
    "rows = int(np.ceil(n / cols))\n",
    "\n",
    "# Crear figura y ejes\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols*5, rows*5))\n",
    "axes = axes.flatten()  # Convertir a lista para iterar f√°cilmente\n",
    "\n",
    "# Lista de colores en gama de rosa\n",
    "pink_palette = [\n",
    "    '#FDE2E4', '#F9C5D1', '#F7A1C4', '#F48FB1', '#F06292',\n",
    "    '#EC407A', '#E91E63', '#D81B60', '#C2185B', '#AD1457',\n",
    "    '#880E4F'\n",
    "]\n",
    "\n",
    "# Crear un gr√°fico de pastel por variable\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    \n",
    "    df_cat[col].value_counts().plot.pie(\n",
    "    ax=axes[i],\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    shadow=True,\n",
    "    explode=[0.05]*df_cat[col].nunique(),\n",
    "    colors=pink_palette[:df_cat[col].nunique()],  # Ajusta al n√∫mero de categor√≠as\n",
    "    textprops={'fontsize': 8}  # Tama√±o de letra m√°s chico\n",
    ")\n",
    "    axes[i].set_title(f'Distribuci√≥n de {col}')\n",
    "    axes[i].set_ylabel('')\n",
    "\n",
    "# Ocultar los ejes extra si hay m√°s subplots que variables\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estudio de valores at√≠picos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Los outliers son datos que se encuentran muy alejados del resto de los valores en una distribuci√≥n.\n",
    "#En estos casos tenemos que hacer limpieza de estos datos que generan ruido.\n",
    "#El estudio se suele contemplar para valores num√©ricos (aunque tambi√©n se puede hacer para valores categ√≥ricos)\n",
    "#Por otro lado, no se pueden considerar outliers en variables binarias (0/1) o (True/False) porque estas variables solo tienen dos posibles valores \n",
    "#y no hay una noci√≥n de ‚Äúvalor extremo‚Äù como en variables continuas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solo vamos a tomar las columnas continuas (prescindiendo de las binarias y que tiene un umbral menor que 4, que no tiene sentido estudiarlas). La variable high_cardinality_cols est√° definida arriba\n",
    "# Crear un sub-dataframe solo con las columnas continuas\n",
    "df_continuous = data[high_cardinality_cols]\n",
    "\n",
    "#Detectar outliers usando IQR\n",
    "Q1 = df_continuous.quantile(0.25)\n",
    "Q3 = df_continuous.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "limite_inferior = Q1 - 1.5 * IQR\n",
    "limite_superior = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = (df_continuous < limite_inferior) | (df_continuous > limite_superior)\n",
    "\n",
    "# Mostrar cantidad de outliers por columna\n",
    "print(\"N√∫mero de outliers por columna:\")\n",
    "print(outliers.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizaci√≥n valores at√≠picos. Diagrama de caja (Boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos el tama√±o general del gr√°fico\n",
    "plt.figure(figsize=(18, 12))  # M√°s grande\n",
    "# Un boxplot por variable, distribuidos en 2 columnas por fila\n",
    "for i, col in enumerate(df_continuous.columns, 1):\n",
    "    plt.subplot((len(df_continuous.columns) + 1) // 2, 2, i)\n",
    "    plt.boxplot(df_continuous[col].dropna(), patch_artist=True,\n",
    "                boxprops=dict(facecolor='#E773B5', color='#E773B5', alpha=0.7),\n",
    "                medianprops=dict(color='white'),\n",
    "                whiskerprops=dict(color='#E773B5'),\n",
    "                capprops=dict(color='#E773B5'),\n",
    "                flierprops=dict(markerfacecolor='#E773B5', marker='o', alpha=0.5))\n",
    "    plt.title(col, fontsize=10)  \n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES\n",
    "\n",
    "#El ajuste de outliers depende de varios factores: si son valores posibles o reales dentro del contexto o si el modelo que vamos a estudiar no es sensible a outliers.\n",
    "#Casos donde conviene tratarlos: si son valores erroneos de entrada (no coincide con la realidad), si se usan modelos sensibles a outliers (como regresi√≥n lineal, KNN y SVM)\n",
    "\n",
    "#age: No tiene outliers. No tenemos que hacer ning√∫n ajuste\n",
    "#trestbps (presi√≥n arterial en reposo): No tiene sentido que un valor para la presi√≥n arterial sea 0 mmHg, sin embargo para un intervalo 80-200 si.\n",
    "    # 80 mmHg puede representar la presi√≥n diast√≥lica normal (cuando el coraz√≥n est√° en reposo).\n",
    "    # 200 mmHg puede representar una presi√≥n sist√≥lica muy alta (como en casos de hipertensi√≥n severa).\n",
    "        #En este caso necesitamos hacer un ajuste para que el 0 no sea tomado como un valor real.\n",
    "#chol (colesterol): No tiene sentido que un valor para el colesterol sea 0 en la vida real, sin embargo s√≠ puede tomar valor 600 mg/dL. Este caso ser√≠a extremadamente alto e indica condici√≥n\n",
    "#m√©dica grave, pero no imposible\n",
    "        #En este caso necesitamos hacer un ajuste para que el 0 no sea tomado como valor real.\n",
    "#thalch: (frecuencia card√≠aca m√°xima en una prueba de esfuerzo): Observamos que hay 2 valores at√≠picos que representan a registros cercanos a 60. Esto puede suceder si la persona no hizo\n",
    "#el esfuerzo suficiente por edad avanzada (efectivamente, tal y como hemos observado en la celda de abajo tras investigarlo). Por lo tanto, no tenemos que hacer ning√∫n ajuste.\n",
    "#oldpeak (diferencia de nivel del segmento ST)\n",
    "        #En este caso necesitamos ajustar los valores negativos que no tienen sentido cl√≠nico.\n",
    "        #Revisamos adem√°s, los valores >6 que son valores muy altos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajuste valor at√≠pico trestbps: \n",
    "#Vemos la cantidad de registros con valor 0 en el campo trestbps\n",
    "data[data['trestbps']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como solo tenemos un caso, podemos eliminarlo:\n",
    "data_drop = data[data['trestbps'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajuste valor at√≠pico chol:\n",
    "#Vemos la cantidad de registros con valor 0 en el campo chol\n",
    "data_drop[data_drop['chol']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como son bastantes (171 registros) vamos a aplicar imputaci√≥n tras convertir los datos de 0 a NaN:\n",
    "# Reemplazar ceros en 'chol' por NaN\n",
    "data_drop.loc[data_drop['chol'] == 0, 'chol'] = np.nan\n",
    "\n",
    "# Imputar con la media o mediana\n",
    "imputer = SimpleImputer(strategy='mean')  # o 'median'\n",
    "data_drop['chol'] = imputer.fit_transform(data_drop[['chol']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observaci√≥n valor thalach:\n",
    "data_drop[data_drop['thalch'] <= 65]\n",
    "#Indicado en la observaci√≥n de arriba: Es normal que los valores at√≠picos para la frecuencia card√≠aca m√°xima est√©n por debajo de 65 bpm (pulsaciones) para estos casos donde las edades son avanzadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajuste valor oldpeak:\n",
    "#Vemos la cantidad de registros con valor negativo en el campo oldpeak\n",
    "data_drop[data_drop['oldpeak']<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tenemos 12 registros. Vamos a pasarlos a NaN e imputar\n",
    "data_drop.loc[data_drop['oldpeak'] < 0, 'oldpeak'] = np.nan\n",
    "# Imputar con la media o mediana\n",
    "imputer = SimpleImputer(strategy='mean')  # o 'median'\n",
    "data_drop['oldpeak'] = imputer.fit_transform(data_drop[['oldpeak']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos la cantidad de registros para valores mayores que 6\n",
    "data_drop[data_drop['oldpeak'] > 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como solo tenemos un caso, podemos eliminarlo:\n",
    "data_1 = data_drop[data_drop['oldpeak'] <= 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurate de usar el dataframe actualizado y solo las columnas num√©ricas continuas\n",
    "df_continuous = data_1[high_cardinality_cols]\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Un boxplot por variable\n",
    "for i, col in enumerate(df_continuous.columns, 1):\n",
    "    plt.subplot((len(df_continuous.columns) + 1) // 2, 2, i)\n",
    "    plt.boxplot(df_continuous[col].dropna(), patch_artist=True,\n",
    "                boxprops=dict(facecolor='#E773B5', color='#E773B5', alpha=0.7),\n",
    "                medianprops=dict(color='white'),\n",
    "                whiskerprops=dict(color='#E773B5'),\n",
    "                capprops=dict(color='#E773B5'),\n",
    "                flierprops=dict(markerfacecolor='#E773B5', marker='o', alpha=0.5))\n",
    "    plt.title(col, fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar DataFrame como ARFF manualmente\n",
    "def dataframe_to_arff(df, filename, relation_name=\"dataset\"):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f\"@RELATION {relation_name}\\n\\n\")\n",
    "        \n",
    "        for col in df.columns:\n",
    "            f.write(f\"@ATTRIBUTE {col} NUMERIC\\n\")\n",
    "        \n",
    "        f.write(\"\\n@DATA\\n\")\n",
    "        for _, row in df.iterrows():\n",
    "            f.write(\",\".join(map(str, row.tolist())) + \"\\n\")\n",
    "\n",
    "# Usar con tu DataFrame\n",
    "dataframe_to_arff(data_1, \"Heart_Attack_2.arff\")\n",
    "print(\"Archivo ARFF generado como 'Heart_Attack_2.arff'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Obtener la ruta a la carpeta Descargas\n",
    "descargas = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "\n",
    "# Guardar el CSV ah√≠\n",
    "df.to_csv(os.path.join(descargas, \"chatgpt2.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de correlaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlaci√≥n\n",
    "correlation_matrix = data_1.corr(numeric_only=True)\n",
    "\n",
    "# Crear un colormap rosa personalizado\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "    \"rose_red_dark\",\n",
    "    ['#E0F8F1', '#ffffff', '#E773B5'],  # suave ‚Üí medio ‚Üí fuerte ‚Üí oscuro\n",
    "    N=256\n",
    ")\n",
    "\n",
    "# Gr√°fico\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=cmap,\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.90}\n",
    ")\n",
    "plt.title('Matriz de Correlaci√≥n', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES:\n",
    "#La matriz utilizada es la correlaci√≥n de Pearson, que mide la realci√≥n lineal entre dos variables.\n",
    "#Los valores van:\n",
    "    #1.0 = correlaci√≥n positiva perfecta (cuando una sube, la otra tambi√©n lo hace en la misma proporci√≥n)\n",
    "    #0 = sin correlaci√≥n lineal (no significa que no haya relaci√≥n, solo que no es lineal)\n",
    "    #-1.0= corrlaci√≥n negativa perfecta (cuando una sube, la otra baja en la misma proporci√≥n)\n",
    "#En nuestra matriz de correlaci√≥n observamos que:\n",
    "#oldpeak y num con 0.4 tienen una correlaci√≥n positiva moderada\n",
    "#oldpeak y exang con 0.4 tienen una correlaci√≥n positiva moderada\n",
    "#exang y num con 0.43 tienen una correlaci√≥n positiva moderada\n",
    "#cp y num con -0.39 tienen una correlaci√≥n negatica moderada\n",
    "\n",
    "#Razones posibles que no inducen a correlaciones fuertes: (>0.7)\n",
    "#La relaci√≥n no es lienal, pues la correlaci√≥n de Pearson solo detecta correlaciones lineales. Si hay una relaci√≥n curva, categ√≥rica o m√°s compleja, no se ver√° reflejada\n",
    "#La variable num (variable objetivo) puede depender de m√∫ltiples factores combinados. Quiz√°s ninguna variable por si sola tenga gran peso, pero en conjunto s√≠ lo tienen(es t√≠pico en modelos de salud y medicina)\n",
    "#Si algunas variables son categ√≥ricas codificadas como n√∫meros puede hacer que la correlaci√≥n se vea afectada\n",
    "#El n√∫mero de muestras es bajo\n",
    "#La limpieza no es completa\n",
    "\n",
    "#Pasos que podemos dar para comprobar la relaci√≥n entre variables:\n",
    "#Visualizar relaciones no lineales (gr√°ficos como pairplot,scatterplot o √°rboles de decisi√≥n)\n",
    "#Probar correlaci√≥n de Spearman (detecta relaciones mon√≥tonas, no lineales)\n",
    "#Hacemos selecci√≥n de variables (usando modelos como Randomforest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar caracter√≠sticas y etiquetas\n",
    "X = data_1.drop(columns='num')\n",
    "y = data_1['num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisi√≥n en entrenamiento/prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES: Al subir el tama√±o del test las precisiones son m√°s altas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selector de variables\n",
    "#Tal y como hemos visto en las observaciones de la matriz de correlaci√≥n, vamos a utilizar el m√©todo de selecci√≥n de variables Randomforest para comprobar la relaci√≥n entre variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar modelo\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "# Importancia de variables\n",
    "importancias = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "importancias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES: \n",
    "# Los n√∫meros que han salido de cada variable representan el peso o contribuci√≥n relativa de cada variable a la hora de hacer predicciones.\n",
    "#No est√°n en escala absoluta (de 0 a 1). Entre todas ellas suman 1 y cuanto m√°s alto, m√°s √∫til es la variable para el modelo.\n",
    "# cp, thalch, chol, oldpeak son las m√°s importantes seg√∫n el modelo.\n",
    "# Podemos sacar como conclusi√≥n que la correlaci√≥n no es identificador de la importancia predictiva en el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M√©todo \"Pairplot\" Modelo no lineal\n",
    "#Vamos a visualizar las variables m√°s importante detectadas anteriormente con Randomforest a trav√©s de las gr√°ficas pairplot (para detectar relaciones no lineales)\n",
    "# Seleccionamos las variables m√°s importantes seg√∫n Random Forest\n",
    "variables_interes = ['cp', 'thalch', 'chol', 'oldpeak', 'age', 'num']\n",
    "sns.pairplot(data_1[variables_interes], hue='num', palette='husl')\n",
    "plt.suptitle(\"Pairplot de variables importantes vs. 'num'\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES:\n",
    "#Efectivamente podemos observar que entre variables no se dan relaciones lineales, se dan curvas y agrupamientos.\n",
    "#num = 0 ‚Üí Cian y num = 1 ‚Üí Rosa\n",
    "#Cada fila y columna es una variable (las seleccionadas por importancia del Random Forest)\n",
    "#Las gr√°ficas de la diagonal son distribuciones (KDEs)\n",
    "#Las otras son scatterplots (gr√°fico de puntos)\n",
    "\n",
    "#Desglose visualizaciones importantes por por variable:\n",
    "#cp (tipo de dolor de pecho)\n",
    "#Distribuci√≥n muy diferente entre clases ‚Üí esto no lo ver√≠a la correlaci√≥n lineal, pero s√≠ Random Forest.\n",
    "\n",
    "#oldpeak (descenso de ST)\n",
    "#num = 1 se agrupa m√°s en valores altos de oldpeak.\n",
    "#Es decir, cuanto mayor es el descenso de ST (oldpeak), m√°s probable es que tenga enfermedad.\n",
    "#Esto tambi√©n es no lineal, ya que no hay una relaci√≥n directa, pero s√≠ una zona cr√≠tica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definici√≥n de modelos\n",
    "models = {\n",
    "    'KNN': make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "    'SVM': make_pipeline(StandardScaler(), SVC()),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'C4.5': DecisionTreeClassifier(criterion='entropy'),\n",
    "    'Neural Network': make_pipeline(\n",
    "        StandardScaler(),\n",
    "        MLPClassifier(hidden_layer_sizes=(30,), max_iter=500, random_state=42)\n",
    "    ),\n",
    "    'Bagging': BaggingClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'Stacking': StackingClassifier(\n",
    "        estimators=[\n",
    "            ('knn', make_pipeline(StandardScaler(), KNeighborsClassifier())),\n",
    "            ('svm', make_pipeline(StandardScaler(), SVC(probability=True))),\n",
    "            ('nb', GaussianNB())\n",
    "        ],\n",
    "        final_estimator=make_pipeline(StandardScaler(), LogisticRegression())\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamiento y evaluaci√≥n\n",
    "accuracy_scores = {}\n",
    "conf_matrices = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy_scores[name] = accuracy_score(y_test, y_pred)\n",
    "    conf_matrices[name] = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f\"{name} Accuracy: {accuracy_scores[name]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a tomar entonces para el estudio de la precisi√≥n de los modelos solo las variables que hemos seleccionado por el m√©todo Randomforest\n",
    "#Comprobemos el comportamiento de la precisiones de los algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_interes = ['cp', 'thalch', 'chol', 'oldpeak', 'age', 'num']\n",
    "data_2 = data_1[variables_interes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar caracter√≠sticas y etiquetas\n",
    "A = data_2.drop(columns='num')\n",
    "b = data_2['num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisi√≥n en entrenamiento/prueba\n",
    "A_train, A_test, b_train, b_test = train_test_split(A, b, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamiento y evaluaci√≥n\n",
    "accuracy_scores_red = {}\n",
    "conf_matrices = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(A_train, b_train)\n",
    "    b_pred = model.predict(A_test)\n",
    "    accuracy_scores_red[name] = accuracy_score(b_test, b_pred)\n",
    "    conf_matrices[name] = confusion_matrix(b_test, b_pred)\n",
    "\n",
    "    print(f\"{name} Accuracy: {accuracy_scores_red[name]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES:\n",
    "#El modelo KNN subi√≥ bastante. Esto es esperable, porque KNN se ve afectado por ruido y variables irrelevantes\n",
    "#C4.5 y AdaBoost tambi√©n mejoraron. Mismos motivos que el caso anterior.\n",
    "#NaiveBayes y Stacking tuvieron peque√±a p√©rdida de precisi√≥n y esto se debe, no necesariamente a algo malo, ya que al reducir variables el modelo es m√°s simple.\n",
    "#Estos modelos necesitan gran cantidad de datos para su estudio. \n",
    "#SVM se mantiene igual, lo que indica que es bastante bueno pues los atributos no aportaban al margen de decisi√≥n de SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "modelos = list(accuracy_scores.keys())\n",
    "todas_vals = [accuracy_scores[m] for m in modelos]\n",
    "reducidas_vals = [accuracy_scores_red[m] for m in modelos]\n",
    "\n",
    "# Crear gr√°fico\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.35\n",
    "index = range(len(modelos))\n",
    "\n",
    "plt.bar(index, todas_vals, bar_width, label='Todas las variables', color='#F08AC0')\n",
    "plt.bar([i + bar_width for i in index], reducidas_vals, bar_width, label='Variables seleccionadas', color='#D63384')\n",
    "\n",
    "plt.xlabel('Modelo')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparaci√≥n de precisi√≥n por modelo')\n",
    "plt.xticks([i + bar_width / 2 for i in index], modelos, rotation=45)\n",
    "plt.ylim(0.6, 0.85)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M√©todo de validaci√≥n cruzada\n",
    "#Vamos a aplicar este m√©todo para comparar con lo anterior.\n",
    "#El m√©todo de validaci√≥n cruzada es un m√©todo para medir la precisi√≥n de forma m√°s robusta. Es m√©todo que sustituye al paso cl√°sico de (train/test).\n",
    "#El m√©todo train/test da una √∫nica precisi√≥n (en test) es decir, tiene una √∫nica divisi√≥n y puede ser enga√±osa si el test es dif√≠cil o f√°cil\n",
    "#El m√©todo de validaci√≥n cruzada entrena y valida K-veces y devuelve el promedio en la precisi√≥n. Esto hace que el m√©todo sea m√°s robusto y fiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M√©todo de validaci√≥n cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular validaci√≥n cruzada\n",
    "accuracy_todas = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=5)\n",
    "    accuracy_todas[name] = scores.mean()\n",
    "\n",
    "# Mostrar resultados ordenados\n",
    "print(\"üìä Accuracy promedio con TODAS las variables:\")\n",
    "for modelo, acc in sorted(accuracy_todas.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{modelo:15s}: {acc * 100:.2f}%\")\n",
    "\n",
    "# (Opcional) Graficar resultados\n",
    "modelos = list(accuracy_todas.keys())\n",
    "valores = [accuracy_todas[m] for m in modelos]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(modelos, valores, color='#E773B5')\n",
    "plt.ylabel(\"Accuracy promedio\")\n",
    "plt.title(\"Validaci√≥n cruzada (5-Fold) con todas las variables\")\n",
    "plt.ylim(0.6, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular validaci√≥n cruzada\n",
    "accuracy_todas_red = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, A, b, cv=5)\n",
    "    accuracy_todas_red[name] = scores.mean()\n",
    "\n",
    "# Mostrar resultados ordenados\n",
    "print(\"üìä Accuracy promedio con las variables seleccionadas:\")\n",
    "for modelo, acc in sorted(accuracy_todas_red.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{modelo:15s}: {acc * 100:.2f}%\")\n",
    "\n",
    "# (Opcional) Graficar resultados\n",
    "modelos = list(accuracy_todas_red.keys())\n",
    "valores = [accuracy_todas_red[m] for m in modelos]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(modelos, valores, color='#E773B5')\n",
    "plt.ylabel(\"Accuracy promedio\")\n",
    "plt.title(\"Validaci√≥n cruzada (5-Fold) con las variables seleccionadas\")\n",
    "plt.ylim(0.6, 1.0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "modelos = list(accuracy_todas.keys())\n",
    "todas_vals = [accuracy_todas[m] for m in modelos]\n",
    "reducidas_vals = [accuracy_todas_red[m] for m in modelos]\n",
    "\n",
    "# Crear gr√°fico\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.35\n",
    "index = range(len(modelos))\n",
    "\n",
    "plt.bar(index, todas_vals, bar_width, label='Todas las variables', color='#F08AC0')\n",
    "plt.bar([i + bar_width for i in index], reducidas_vals, bar_width, label='Variables seleccionadas', color='#D63384')\n",
    "\n",
    "plt.xlabel('Modelo')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparaci√≥n de precisi√≥n por modelo. Validaci√≥n cruzada')\n",
    "plt.xticks([i + bar_width / 2 for i in index], modelos, rotation=45)\n",
    "plt.ylim(0.6, 0.85)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES:\n",
    "#Las observaciones para el m√©todo de validaci√≥n cruzada son m√°s realistas y menos optimistas del rendimiento del modelo. Representan mejor el comportamiento en general.\n",
    "#Representa el rendimiento real del modelo\n",
    "#Que las precisiones hayan bajado es porque ha evitado el sobreajuste al modelo de train/test. \n",
    "#Con validaci√≥n cruzada el modelo se evalua varias veces sobre diferenes particiones, eliminando la suerte ocasional.\n",
    "#Reduce el riesgo de overfiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gr√°fica de comparaci√≥n de precisi√≥n por modelo:Train/Test vs Validaci√≥n Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos = list(accuracy_scores.keys())\n",
    "x = range(len(modelos))\n",
    "bar_width = 0.2\n",
    "\n",
    "# Crear gr√°fico combinado\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Train/test\n",
    "plt.bar([i - 1.5 * bar_width for i in x], [accuracy_scores[m] for m in modelos],\n",
    "        width=bar_width, label='Train/Test - Todas', color='#A4C8E1')\n",
    "\n",
    "plt.bar([i - 0.5 * bar_width for i in x], [accuracy_scores_red[m] for m in modelos],\n",
    "        width=bar_width, label='Train/Test - Seleccionadas', color='#6497B1')\n",
    "\n",
    "# Validaci√≥n cruzada\n",
    "plt.bar([i + 0.5 * bar_width for i in x], [accuracy_todas[m] for m in modelos],\n",
    "        width=bar_width, label='CV - Todas', color='#F08AC0')\n",
    "\n",
    "plt.bar([i + 1.5 * bar_width for i in x], [accuracy_todas_red[m] for m in modelos],\n",
    "        width=bar_width, label='CV - Seleccionadas', color='#D63384')\n",
    "\n",
    "# Ajustes\n",
    "plt.xlabel('Modelo')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparaci√≥n de precisi√≥n por modelo: Train/Test vs Validaci√≥n Cruzada')\n",
    "plt.xticks(x, modelos, rotation=45)\n",
    "plt.ylim(0.6, 0.85)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBSERVACIONES:\n",
    "#Modelos √°rbol como C4.5 y Bagging tienen a ajustarse al dataset de entrenamiento si no se podan o se regularizan bien. Por eso, cuando se prueban en validaci√≥n cruzada, con varias particiones,\n",
    "#su rendimiento cae m√°s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consideraciones generales (Modelos Algoritmos)\n",
    "# Porcentaje de test/train:\n",
    "    # Lo est√°ndar es 80% train / 20% test o usar validaci√≥n cruzada (cv=5 o cv=10).\n",
    "    # Modelos como Neural Network y SVM se benefician de m√°s datos para entrenar.\n",
    "\n",
    "# Tratamiento de outliers:\n",
    "    # Muy importante para KNN y redes neuronales.\n",
    "    # √Årboles y bagging los manejan mejor por dise√±o.\n",
    "\n",
    "# Ruido en los datos:\n",
    "    # KNN, redes y √°rboles lo sufren m√°s.\n",
    "    # Bagging y Naive Bayes lo toleran mejor.\n",
    "\n",
    "# Normalizaci√≥n:\n",
    "    # Crucial para todos los algoritmos basados en distancia o gradientes: KNN, SVM, Redes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tfg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
